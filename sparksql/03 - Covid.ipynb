{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trentw0826/covid-sparksql-analysis/blob/main/sparksql/03%20-%20Covid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZfzbTFIH_hY9",
        "outputId": "0d34c483-ad6a-48bc-8d1d-602e9aebd477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxt-dev libxtst6 libxxf86dga1\n",
            "  openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
            "  session-migration x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxt-dev libxtst6 libxxf86dga1\n",
            "  openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
            "  openjdk-11-jre-headless session-migration x11-utils\n",
            "0 upgraded, 20 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 122 MB of archives.\n",
            "After this operation, 275 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Ign:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Ign:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Ign:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Ign:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Err:17 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 91.189.91.82 80]\n",
            "Err:18 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 91.189.91.82 80]\n",
            "Err:19 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 91.189.91.82 80]\n",
            "Err:20 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 91.189.91.82 80]\n",
            "Fetched 4,117 kB in 1s (7,517 kB/s)\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jre-headless_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 91.189.91.82 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jre_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 91.189.91.82 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk-headless_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 91.189.91.82 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 91.189.91.82 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ],
      "source": [
        "# Setup Java\n",
        "!apt-get install openjdk-11-jdk -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BTogu7y3WuMV",
        "outputId": "e124f229-1932-4877-e11d-799b11db09cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "# Setup Spark SQL\n",
        "# Note if running locally you need the JVM https://www.oracle.com/java/technologies/downloads/\n",
        "# Also, if running locally you'll need to allow it to talk over the network to your own machine\n",
        "# Consider running in https://colab.research.google.com/\n",
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8zjIATulWuMX"
      },
      "outputs": [],
      "source": [
        "# Initialize Context - this is where you'd setup information about your Hadoop cluster if you had one!\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Covid\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "sc.setLogLevel(\"WARN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0c_HeLoEWuMY",
        "outputId": "05e698a4-eaac-410e-8408-436c74dfc9a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 99.9M  100 99.9M    0     0  37.1M      0  0:00:02  0:00:02 --:--:-- 37.1M\n"
          ]
        }
      ],
      "source": [
        "# Download 100mb covid county data file\n",
        "!curl \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\" > ./uscounties.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLqqsUF4WuMY"
      },
      "outputs": [],
      "source": [
        "# Write code to define or infer the schema and then read in the dataset\n",
        "# Read the file into a Spark DataFrame\n",
        "usCountiesFilePath = \"./uscounties.csv\"\n",
        "from pyspark.sql.types import DateType, StringType, StructField, StructType, IntegerType\n",
        "\n",
        "# the fips column needs to be a string in order to preserve leading zeros\n",
        "schema = StructType([StructField('date', DateType(), True), StructField('county', StringType(), True), StructField('state', StringType(), True), StructField('fips', StringType(), True), StructField('cases', IntegerType(), True), StructField('deaths', IntegerType(), True)])\n",
        "\n",
        "df = spark.read.csv(usCountiesFilePath, schema=schema, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJZ0D9BSWuMY"
      },
      "outputs": [],
      "source": [
        "# Write code to find the county with the most deaths\n",
        "# Option 1) SparkSQL API\n",
        "df.createOrReplaceTempView(\"covid\")  # create table that you can do sql on\n",
        "\n",
        "print(\"Max deaths:\")\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    select county, state, deaths\n",
        "    from covid\n",
        "    order by deaths desc\n",
        "    limit 1\n",
        "  \"\"\"\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un4dLr7SWuMY"
      },
      "outputs": [],
      "source": [
        "# Option 2) # DataFrame style\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "print(\"Max deaths:\")\n",
        "print(\n",
        "    df.orderBy(col(\"deaths\").desc()).take(  # .where(col(\"county\") == \"New York City\") \\\n",
        "        1\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tnf9daUWuMZ"
      },
      "outputs": [],
      "source": [
        "# # Option 3) RDD MapReduce Style without key\n",
        "rows = df.rdd\n",
        "\n",
        "\n",
        "def getMax(cumm, other):\n",
        "    if other[\"deaths\"] is not None and other[\"deaths\"] > cumm[\"deaths\"]:\n",
        "        return other\n",
        "    else:\n",
        "        return cumm\n",
        "\n",
        "\n",
        "print(\"Max deaths:\")\n",
        "print(rows.reduce(getMax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Niq8SodKWuMZ"
      },
      "outputs": [],
      "source": [
        "# # Option 4) RDD MapReduce Style with mapped tuples\n",
        "rows = df.rdd\n",
        "\n",
        "\n",
        "def getMax(cumm, other):\n",
        "    if other[0] > cumm[0]:\n",
        "        return other\n",
        "    else:\n",
        "        return cumm\n",
        "\n",
        "\n",
        "rows = rows.map(lambda r: (r[\"deaths\"] or 0, f\"{r['county']},{r['state']}\"))\n",
        "print(\"Max deaths:\")\n",
        "print(rows.reduce(getMax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag3VUpo9WuMZ"
      },
      "outputs": [],
      "source": [
        "# Write code to find the county with the most deaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASwuQqrnWuMZ"
      },
      "outputs": [],
      "source": [
        "# Write code to find the county with the most cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xEfrgZ4WuMZ"
      },
      "outputs": [],
      "source": [
        "# Write code to find the total number of deaths in Utah county"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTbHRoiLWuMZ"
      },
      "outputs": [],
      "source": [
        "# Write code to find the death rate for each state and sort the states by death rate descending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2pPvp5VWuMZ"
      },
      "outputs": [],
      "source": [
        "# Write code to something else interesting with this data â€“ your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HASapzHvXF5a"
      },
      "outputs": [],
      "source": [
        "# Extra Credit 1 - Plot your death rate data!\n",
        "# Extra Credit 2 - Join this with other data or find something intresting in this data and plot it on a map!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4tCU11zhjJY"
      },
      "outputs": [],
      "source": [
        "# This example uses two-letter state code\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "data = pd.DataFrame({\n",
        "  'state': ['NY', 'CA', 'TX', 'FL'],\n",
        "  'values': [10, 20, 15, 25]\n",
        "})\n",
        "\n",
        "fig = px.choropleth(\n",
        "    data,\n",
        "    locations='state', # Column with state abbreviations\n",
        "    locationmode='USA-states', # Set location mode to US states\n",
        "    color='values', # Column to determine color intensity\n",
        "    scope='usa', # Limit map to the USA\n",
        "    color_continuous_scale='Viridis', # Choose a color scale\n",
        "    title='Extra Credit Plot <Insert name here>'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjH6Dq8saDY8"
      },
      "outputs": [],
      "source": [
        "# This example uses the FIPS code to map data to a county\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Example county-level data (FIPS codes are required for county-level plots)\n",
        "data = pd.DataFrame({\n",
        "    'fips': ['36061', '06037', '48201', '12086'],  # Example FIPS codes (NYC, LA, Houston, Miami-Dade)\n",
        "    'values': [10, 20, 15, 25]\n",
        "})\n",
        "\n",
        "# Plot county-level choropleth map\n",
        "fig = px.choropleth(\n",
        "    data,\n",
        "    geojson=\"https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json\",  # GeoJSON for counties\n",
        "    locations='fips',  # Use county FIPS codes\n",
        "    color='values',  # Column to determine color intensity\n",
        "    color_continuous_scale='Viridis',\n",
        "    scope='usa',\n",
        "    title='County-Level Extra Credit Plot'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}